{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b63b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5125853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0025\n",
    "data_size = [100, 300, 600, 900, 1200, 1500]\n",
    "\n",
    "normal_dir = 'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CAE_dataset\\\\normal'\n",
    "abnormal_dir = 'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CAE_dataset\\\\abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2706f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "normal_images = datasets.ImageFolder(\n",
    "    normal_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_images = datasets.ImageFolder(\n",
    "    abnormal_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(len(normal_images))\n",
    "print(len(abnormal_images))\n",
    "\n",
    "train_normal_images, test_normal_images = torch.utils.data.random_split(normal_images, [int(len(normal_images)*0.8), int(len(normal_images)*0.2)])\n",
    "train_abnormal_images, test_abnormal_images = torch.utils.data.random_split(abnormal_images, [int(len(abnormal_images)*0.8), int(len(abnormal_images)*0.2)])\n",
    "\n",
    "labeled_train_abnormal_images = []\n",
    "for feature, index in train_abnormal_images:\n",
    "    labeled_train_abnormal_images.append([feature, 1])\n",
    "\n",
    "labeled_test_abnormal_images = []\n",
    "for feature, index in test_abnormal_images:\n",
    "    labeled_test_abnormal_images.append([feature, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99d60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5) \n",
    "        self.conv3 = nn.Conv2d(12, 24, 4)  \n",
    "        self.fc1 = nn.Linear(24 * 25 * 25, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # 6 * 110 * 110\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 12 * 53 * 53\n",
    "        x = self.pool(F.relu(self.conv3(x))) # 24 * 25 * 25\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640c200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abnormal data : 100\n",
      "43\n",
      "10\n",
      "[0/50] Train loss : 0.6653978263222894 Test loss : 0.6714873671531677\n",
      "[10/50] Train loss : 0.5689975499760273 Test loss : 0.6909897327423096\n",
      "[20/50] Train loss : 0.3534376678858386 Test loss : 1.5763522386550903\n",
      "[30/50] Train loss : 0.23874003769353377 Test loss : 1.771200430393219\n",
      "[40/50] Train loss : 0.17107356103613627 Test loss : 1.6052270889282227\n",
      "save model CNN_model/CNN_best_model_0.pth \n",
      "\n",
      "Number of abnormal data : 300\n",
      "68\n",
      "15\n",
      "[0/50] Train loss : 0.6881053544142667 Test loss : 0.6940934499104817\n",
      "[10/50] Train loss : 0.6791799388387624 Test loss : 0.7173091530799866\n",
      "[20/50] Train loss : 0.38787815782844143 Test loss : 1.7490138173103333\n",
      "[30/50] Train loss : 0.22560231446507661 Test loss : 1.8809935119003058\n",
      "[40/50] Train loss : 0.16524986917262569 Test loss : 1.8143692096074422\n",
      "save model CNN_model/CNN_best_model_1.pth \n",
      "\n",
      "Number of abnormal data : 600\n",
      "105\n",
      "23\n",
      "[0/50] Train loss : 0.4259268543904736 Test loss : 1.36332657115291\n",
      "[10/50] Train loss : 0.2592625136753278 Test loss : 1.601272910732128\n",
      "[20/50] Train loss : 0.14887843580606083 Test loss : 1.5331329680250392\n",
      "[30/50] Train loss : 0.1061967729768228 Test loss : 1.4648291888322844\n",
      "[40/50] Train loss : 0.07840493989975325 Test loss : 1.2346225619194624\n",
      "save model CNN_model/CNN_best_model_2.pth \n",
      "\n",
      "Number of abnormal data : 900\n",
      "143\n",
      "30\n",
      "[0/50] Train loss : 0.6179911742260406 Test loss : 0.5605014850695927\n",
      "[10/50] Train loss : 0.20157452183287147 Test loss : 1.363769456759716\n",
      "[20/50] Train loss : 0.0825241018376489 Test loss : 1.1935979558698213\n",
      "[30/50] Train loss : 0.04121565713422614 Test loss : 0.9009007235368093\n",
      "[40/50] Train loss : 0.016187767075172013 Test loss : 0.4400677773238082\n",
      "save model CNN_model/CNN_best_model_3.pth \n",
      "\n",
      "Number of abnormal data : 1200\n",
      "180\n",
      "38\n",
      "[0/50] Train loss : 0.4491973989177495 Test loss : 0.9084599332540835\n",
      "[10/50] Train loss : 0.1497409418095938 Test loss : 1.1517375613728196\n",
      "[20/50] Train loss : 0.0639031050341954 Test loss : 1.0627347374500364\n",
      "[30/50] Train loss : 0.04693807399032974 Test loss : 0.9918882676161287\n",
      "[40/50] Train loss : 0.030207592124336064 Test loss : 0.689105475531266\n",
      "save model CNN_model/CNN_best_model_4.pth \n",
      "\n",
      "Number of abnormal data : 1500\n",
      "218\n",
      "45\n",
      "[0/50] Train loss : 0.43331444395466817 Test loss : 0.7546338947075937\n",
      "[10/50] Train loss : 0.15839387795411547 Test loss : 1.041154476454378\n",
      "[20/50] Train loss : 0.0797517936831071 Test loss : 0.9676211330405851\n",
      "[30/50] Train loss : 0.05072584206589771 Test loss : 0.8649706296765038\n",
      "[40/50] Train loss : 0.03477971317237745 Test loss : 0.7467373773436218\n",
      "save model CNN_model/CNN_best_model_5.pth \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for number, dsize in enumerate(data_size, 0):\n",
    "    \n",
    "    CNN_PATH = f'CNN_model/CNN_best_model_{number}.pth'\n",
    "    \n",
    "    print(f\"Number of abnormal data : {dsize}\")\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_normal_images + labeled_train_abnormal_images[:dsize], batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_normal_images + labeled_test_abnormal_images[:int(dsize*0.2)], batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    print(len(train_loader))\n",
    "    print(len(test_loader))\n",
    "    \n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = cnn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(test_loader, 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = cnn(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"[{epoch}/{num_epochs}] Train loss : {train_loss / len(train_loader)} Test loss : {test_loss/len(test_loader)}\")\n",
    "\n",
    "    torch.save(cnn.state_dict(), CNN_PATH)\n",
    "    print(\"save model \" + CNN_PATH ,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a05caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106377d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
