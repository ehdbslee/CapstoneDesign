{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540bae9d",
   "metadata": {},
   "source": [
    "**dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7314ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af7a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 40\n",
    "learning_rate = 0.002\n",
    "threshold = 0.0011679699386776773\n",
    "\n",
    "cnn_err_rate = []\n",
    "ae_err_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6c6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset10',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader1 = torch.utils.data.DataLoader(dataset1, batch_size=batch_size)\n",
    "test_loader1 = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "#for data, label in dataset1:\n",
    " #   print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "223fe3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset30',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader2 = torch.utils.data.DataLoader(dataset2, batch_size=batch_size)\n",
    "test_loader2 = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef99ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset60',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader3 = torch.utils.data.DataLoader(dataset3, batch_size=batch_size)\n",
    "test_loader3 = torch.utils.data.DataLoader(dataset3, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0896731",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset90',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader4 = torch.utils.data.DataLoader(dataset4, batch_size=batch_size)\n",
    "test_loader4 = torch.utils.data.DataLoader(dataset4, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a496974",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset120',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader5 = torch.utils.data.DataLoader(dataset5, batch_size=batch_size)\n",
    "test_loader5 = torch.utils.data.DataLoader(dataset5, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae7cbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset150',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader6 = torch.utils.data.DataLoader(dataset6, batch_size=batch_size)\n",
    "test_loader6 = torch.utils.data.DataLoader(dataset6, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1dfc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset7 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset230',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader7 = torch.utils.data.DataLoader(dataset7, batch_size=batch_size)\n",
    "test_loader7 = torch.utils.data.DataLoader(dataset7, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c7a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset8 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\CNN_dataset300',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader8 = torch.utils.data.DataLoader(dataset8, batch_size=batch_size)\n",
    "test_loader8 = torch.utils.data.DataLoader(dataset8, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459373d",
   "metadata": {},
   "source": [
    "**CNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1572009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5) \n",
    "        self.conv3 = nn.Conv2d(12, 24, 4)  \n",
    "        self.fc1 = nn.Linear(24 * 25 * 25, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # 6 * 110 * 110\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 12 * 53 * 53\n",
    "        x = self.pool(F.relu(self.conv3(x))) # 24 * 25 * 25\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "cnn1 = CNN()\n",
    "cnn2 = CNN()\n",
    "cnn3 = CNN()\n",
    "cnn4 = CNN()\n",
    "cnn5 = CNN()\n",
    "cnn6 = CNN()\n",
    "cnn7 = CNN()\n",
    "cnn8 = CNN()\n",
    "\n",
    "cnn1 = cnn1.cuda()\n",
    "cnn2 = cnn2.cuda()\n",
    "cnn3 = cnn3.cuda()\n",
    "cnn4 = cnn4.cuda()\n",
    "cnn5 = cnn5.cuda()\n",
    "cnn6 = cnn6.cuda()\n",
    "cnn7 = cnn7.cuda()\n",
    "cnn8 = cnn8.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "894c6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.SGD(cnn1.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.SGD(cnn2.parameters(), lr=learning_rate)\n",
    "optimizer3 = torch.optim.SGD(cnn3.parameters(), lr=learning_rate)\n",
    "optimizer4 = torch.optim.SGD(cnn4.parameters(), lr=learning_rate)\n",
    "optimizer5 = torch.optim.SGD(cnn5.parameters(), lr=learning_rate)\n",
    "optimizer6 = torch.optim.SGD(cnn6.parameters(), lr=learning_rate)\n",
    "optimizer7 = torch.optim.SGD(cnn7.parameters(), lr=learning_rate)\n",
    "optimizer8 = torch.optim.SGD(cnn8.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b0ab0",
   "metadata": {},
   "source": [
    "***CAE model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73390e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set1 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal10',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader1 = torch.utils.data.DataLoader(abnormal_set1, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4edc8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set2 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal30',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader2 = torch.utils.data.DataLoader(abnormal_set2, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79154827",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set3 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal60',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader3 = torch.utils.data.DataLoader(abnormal_set3, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41e97039",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set4 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal90',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader4 = torch.utils.data.DataLoader(abnormal_set4, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0922249",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set5 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal120',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader5 = torch.utils.data.DataLoader(abnormal_set5, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e356fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set6 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal150',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader6 = torch.utils.data.DataLoader(abnormal_set6, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efd0d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set7 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal230',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader7 = torch.utils.data.DataLoader(abnormal_set7, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a346ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_set8 = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\WorkStation\\\\Desktop\\\\캡디이미지\\\\abnormal300',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "abnormal_loader8 = torch.utils.data.DataLoader(abnormal_set8, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "293c28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                         nn.MaxPool2d(2,2))\n",
    "\n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "                                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.cnn_layer3 = nn.Sequential(\n",
    "                                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2,2))\n",
    "\n",
    "        # Decoder        \n",
    "        self.tran_cnn_layer1 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(64, 32, kernel_size = 2, stride = 2, padding=0),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.tran_cnn_layer2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(32, 16, kernel_size = 2, stride = 2, padding=0),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.tran_cnn_layer3 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(16, 3, kernel_size = 2, stride = 2, padding=0),\n",
    "                        nn.Sigmoid())  \n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = self.cnn_layer1(x)\n",
    "        output = self.cnn_layer2(output)\n",
    "        output = self.cnn_layer3(output)        \n",
    "        output = self.tran_cnn_layer1(output)\n",
    "        output = self.tran_cnn_layer2(output)\n",
    "        output = self.tran_cnn_layer3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da949680",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = ConvAutoEncoder()\n",
    "AE.load_state_dict(torch.load('best_model_final.pth'))\n",
    "ae_criterion = nn.MSELoss()\n",
    "AE = AE.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603f783",
   "metadata": {},
   "source": [
    "***train(10)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb958247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 0.3891297\n",
      "[1,    40] loss: 3.3202375\n",
      "[2,    40] loss: 0.6439550\n",
      "[3,    40] loss: 0.4619604\n",
      "[4,    40] loss: 0.5278840\n",
      "[5,    40] loss: 0.4670880\n",
      "[6,    40] loss: 0.4489288\n",
      "[7,    40] loss: 0.4491248\n",
      "[8,    40] loss: 0.4543119\n",
      "[9,    40] loss: 0.4537917\n",
      "[10,    40] loss: 0.4479744\n",
      "[11,    40] loss: 0.4453162\n",
      "[12,    40] loss: 0.4454958\n",
      "[13,    40] loss: 0.4450632\n",
      "[14,    40] loss: 0.4434886\n",
      "[15,    40] loss: 0.4416584\n",
      "[16,    40] loss: 0.4396521\n",
      "[17,    40] loss: 0.4361106\n",
      "[18,    40] loss: 0.4421771\n",
      "[19,    40] loss: 0.4481117\n",
      "[20,    40] loss: 0.4286830\n",
      "[21,    40] loss: 0.4079871\n",
      "[22,    40] loss: 0.4479164\n",
      "[23,    40] loss: 0.4084883\n",
      "[24,    40] loss: 0.3854104\n",
      "[25,    40] loss: 0.3705044\n",
      "[26,    40] loss: 0.3428984\n",
      "[27,    40] loss: 0.3134645\n",
      "[28,    40] loss: 0.2350283\n",
      "[29,    40] loss: 0.1979423\n",
      "[30,    40] loss: 0.1084272\n",
      "[31,    40] loss: 0.0616375\n",
      "[32,    40] loss: 0.0041925\n",
      "[33,    40] loss: 0.0003941\n",
      "[34,    40] loss: 0.0000834\n",
      "[35,    40] loss: 0.0000450\n",
      "[36,    40] loss: 0.0000344\n",
      "[37,    40] loss: 0.0000298\n",
      "[38,    40] loss: 0.0000272\n",
      "[39,    40] loss: 0.0000257\n",
      "Finished Training\n",
      "Error rate of test images: 0.000000\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader1, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer1.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn1(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader1)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader1:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn1(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader1):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c0960",
   "metadata": {},
   "source": [
    "***train(30)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4de8cef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 0.9226418\n",
      "[1,    40] loss: 0.7199709\n",
      "[2,    40] loss: 0.7430887\n",
      "[3,    40] loss: 0.6852600\n",
      "[4,    40] loss: 0.6770119\n",
      "[5,    40] loss: 0.6760054\n",
      "[6,    40] loss: 0.6784320\n",
      "[7,    40] loss: 0.6802162\n",
      "[8,    40] loss: 0.6801984\n",
      "[9,    40] loss: 0.6795553\n",
      "[10,    40] loss: 0.6777913\n",
      "[11,    40] loss: 0.6770246\n",
      "[12,    40] loss: 0.6767584\n",
      "[13,    40] loss: 0.6764854\n",
      "[14,    40] loss: 0.6759902\n",
      "[15,    40] loss: 0.6754848\n",
      "[16,    40] loss: 0.6751560\n",
      "[17,    40] loss: 0.6748533\n",
      "[18,    40] loss: 0.6744816\n",
      "[19,    40] loss: 0.6744734\n",
      "[20,    40] loss: 0.6741588\n",
      "[21,    40] loss: 0.6737650\n",
      "[22,    40] loss: 0.6734695\n",
      "[23,    40] loss: 0.6731139\n",
      "[24,    40] loss: 0.6732622\n",
      "[25,    40] loss: 0.6728353\n",
      "[26,    40] loss: 0.6730187\n",
      "[27,    40] loss: 0.6725460\n",
      "[28,    40] loss: 0.6723456\n",
      "[29,    40] loss: 0.6721058\n",
      "[30,    40] loss: 0.6719988\n",
      "[31,    40] loss: 0.6717082\n",
      "[32,    40] loss: 0.6717433\n",
      "[33,    40] loss: 0.6714784\n",
      "[34,    40] loss: 0.6712577\n",
      "[35,    40] loss: 0.6710402\n",
      "[36,    40] loss: 0.6707623\n",
      "[37,    40] loss: 0.6705729\n",
      "[38,    40] loss: 0.6704067\n",
      "[39,    40] loss: 0.6702574\n",
      "Finished Training\n",
      "Error rate of test images: 0.375000\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader2, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn2(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader2)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader2:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn2(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader2):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93757d63",
   "metadata": {},
   "source": [
    "***train(60)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c175de71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 10.5261873\n",
      "[1,    40] loss: 0.7583981\n",
      "[2,    40] loss: 0.7029720\n",
      "[3,    40] loss: 0.7010460\n",
      "[4,    40] loss: 0.6993170\n",
      "[5,    40] loss: 0.7001373\n",
      "[6,    40] loss: 0.6980537\n",
      "[7,    40] loss: 0.6965504\n",
      "[8,    40] loss: 0.6966390\n",
      "[9,    40] loss: 0.6962443\n",
      "[10,    40] loss: 0.6958631\n",
      "[11,    40] loss: 0.6955501\n",
      "[12,    40] loss: 0.6952697\n",
      "[13,    40] loss: 0.6950799\n",
      "[14,    40] loss: 0.6948955\n",
      "[15,    40] loss: 0.6946523\n",
      "[16,    40] loss: 0.6945246\n",
      "[17,    40] loss: 0.6944168\n",
      "[18,    40] loss: 0.6943557\n",
      "[19,    40] loss: 0.6941831\n",
      "[20,    40] loss: 0.6940576\n",
      "[21,    40] loss: 0.6939603\n",
      "[22,    40] loss: 0.6937488\n",
      "[23,    40] loss: 0.6939143\n",
      "[24,    40] loss: 0.6936284\n",
      "[25,    40] loss: 0.6935297\n",
      "[26,    40] loss: 0.6934325\n",
      "[27,    40] loss: 0.6933396\n",
      "[28,    40] loss: 0.6932479\n",
      "[29,    40] loss: 0.6931933\n",
      "[30,    40] loss: 0.6930329\n",
      "[31,    40] loss: 0.6934863\n",
      "[32,    40] loss: 0.6930172\n",
      "[33,    40] loss: 0.6929588\n",
      "[34,    40] loss: 0.6928964\n",
      "[35,    40] loss: 0.6928447\n",
      "[36,    40] loss: 0.6927977\n",
      "[37,    40] loss: 0.6927551\n",
      "[38,    40] loss: 0.6931615\n",
      "[39,    40] loss: 0.6939179\n",
      "Finished Training\n",
      "Error rate of test images: 0.454545\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader3, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer3.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn3(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader3)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader3:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn3(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader3):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77059cf",
   "metadata": {},
   "source": [
    "***train(90)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3089693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 24.5492307\n",
      "[1,    40] loss: 0.9413199\n",
      "[2,    40] loss: 0.7079849\n",
      "[3,    40] loss: 0.6905462\n",
      "[4,    40] loss: 0.6839935\n",
      "[5,    40] loss: 0.6863220\n",
      "[6,    40] loss: 0.5740802\n",
      "[7,    40] loss: 0.5234560\n",
      "[8,    40] loss: 0.1230372\n",
      "[9,    40] loss: 1.6547098\n",
      "[10,    40] loss: 0.6245434\n",
      "[11,    40] loss: 0.6263852\n",
      "[12,    40] loss: 0.5696360\n",
      "[13,    40] loss: 0.5448324\n",
      "[14,    40] loss: 0.5405204\n",
      "[15,    40] loss: 0.5254517\n",
      "[16,    40] loss: 0.4925862\n",
      "[17,    40] loss: 0.4359133\n",
      "[18,    40] loss: 0.3502016\n",
      "[19,    40] loss: 0.2918083\n",
      "[20,    40] loss: 0.2368154\n",
      "[21,    40] loss: 0.2027620\n",
      "[22,    40] loss: 0.1652654\n",
      "[23,    40] loss: 0.1213356\n",
      "[24,    40] loss: 0.0770675\n",
      "[25,    40] loss: 0.0346921\n",
      "[26,    40] loss: 0.0067676\n",
      "[27,    40] loss: 0.0029025\n",
      "[28,    40] loss: 0.0012195\n",
      "[29,    40] loss: 0.0008984\n",
      "[30,    40] loss: 0.0007222\n",
      "[31,    40] loss: 0.0005927\n",
      "[32,    40] loss: 0.0004998\n",
      "[33,    40] loss: 0.0004291\n",
      "[34,    40] loss: 0.0003728\n",
      "[35,    40] loss: 0.0003270\n",
      "[36,    40] loss: 0.0002893\n",
      "[37,    40] loss: 0.0002577\n",
      "[38,    40] loss: 0.0002311\n",
      "[39,    40] loss: 0.0002086\n",
      "Finished Training\n",
      "Error rate of test images: 0.000000\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader4, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer4.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn4(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer4.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader4)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader4:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn4(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader4):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b14690",
   "metadata": {},
   "source": [
    "***train(120)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec427633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 54.6130350\n",
      "[1,    40] loss: 0.8074916\n",
      "[2,    40] loss: 0.7353884\n",
      "[3,    40] loss: 0.6714759\n",
      "[4,    40] loss: 0.6839832\n",
      "[5,    40] loss: 0.6779714\n",
      "[6,    40] loss: 0.6686801\n",
      "[7,    40] loss: 0.6665517\n",
      "[8,    40] loss: 0.6643086\n",
      "[9,    40] loss: 0.6620925\n",
      "[10,    40] loss: 0.6591408\n",
      "[11,    40] loss: 0.6581839\n",
      "[12,    40] loss: 0.6557170\n",
      "[13,    40] loss: 0.6774525\n",
      "[14,    40] loss: 0.6439601\n",
      "[15,    40] loss: 1.7657246\n",
      "[16,    40] loss: 0.7977092\n",
      "[17,    40] loss: 0.5704952\n",
      "[18,    40] loss: 0.5548940\n",
      "[19,    40] loss: 0.5280824\n",
      "[20,    40] loss: 1.1035325\n",
      "[21,    40] loss: 0.6761996\n",
      "[22,    40] loss: 0.6724248\n",
      "[23,    40] loss: 0.6657297\n",
      "[24,    40] loss: 0.6605035\n",
      "[25,    40] loss: 0.6559903\n",
      "[26,    40] loss: 0.6517554\n",
      "[27,    40] loss: 0.6480666\n",
      "[28,    40] loss: 0.6450009\n",
      "[29,    40] loss: 0.6425100\n",
      "[30,    40] loss: 0.6406153\n",
      "[31,    40] loss: 0.6391143\n",
      "[32,    40] loss: 0.6379327\n",
      "[33,    40] loss: 0.6370214\n",
      "[34,    40] loss: 0.6362932\n",
      "[35,    40] loss: 0.6357108\n",
      "[36,    40] loss: 0.6352235\n",
      "[37,    40] loss: 0.6348378\n",
      "[38,    40] loss: 0.6344953\n",
      "[39,    40] loss: 0.6342085\n",
      "Finished Training\n",
      "Error rate of test images: 0.294118\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader5, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer5.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn5(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer5.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader5)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader5:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn5(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader5):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b722440",
   "metadata": {},
   "source": [
    "***train(180)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48e25b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 45.6428745\n",
      "[1,    40] loss: 1.5081129\n",
      "[2,    40] loss: 0.7027723\n",
      "[3,    40] loss: 0.6969057\n",
      "[4,    40] loss: 0.6636187\n",
      "[5,    40] loss: 0.6317341\n",
      "[6,    40] loss: 0.6083374\n",
      "[7,    40] loss: 0.5946561\n",
      "[8,    40] loss: 0.5874307\n",
      "[9,    40] loss: 0.5836018\n",
      "[10,    40] loss: 0.5814925\n",
      "[11,    40] loss: 0.5802107\n",
      "[12,    40] loss: 0.5793567\n",
      "[13,    40] loss: 0.5787703\n",
      "[14,    40] loss: 0.5783133\n",
      "[15,    40] loss: 0.5779868\n",
      "[16,    40] loss: 0.5776950\n",
      "[17,    40] loss: 0.5774893\n",
      "[18,    40] loss: 0.5772795\n",
      "[19,    40] loss: 0.5771100\n",
      "[20,    40] loss: 0.5769759\n",
      "[21,    40] loss: 0.5768233\n",
      "[22,    40] loss: 0.5767027\n",
      "[23,    40] loss: 0.5765968\n",
      "[24,    40] loss: 0.5764914\n",
      "[25,    40] loss: 0.5763908\n",
      "[26,    40] loss: 0.5763095\n",
      "[27,    40] loss: 0.5762161\n",
      "[28,    40] loss: 0.5761378\n",
      "[29,    40] loss: 0.5760537\n",
      "[30,    40] loss: 0.5759789\n",
      "[31,    40] loss: 0.5759272\n",
      "[32,    40] loss: 0.5758466\n",
      "[33,    40] loss: 0.5757804\n",
      "[34,    40] loss: 0.5757199\n",
      "[35,    40] loss: 0.5756374\n",
      "[36,    40] loss: 0.5755785\n",
      "[37,    40] loss: 0.5755259\n",
      "[38,    40] loss: 0.5754696\n",
      "[39,    40] loss: 0.5754107\n",
      "Finished Training\n",
      "Error rate of test images: 0.250000\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader6, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer6.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn6(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer6.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader6)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader6:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn6(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader6):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7dc27b",
   "metadata": {},
   "source": [
    "***train(230)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cc40985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 34.4224477\n",
      "[1,    40] loss: 2.5363900\n",
      "[2,    40] loss: 0.6986645\n",
      "[3,    40] loss: 0.5009905\n",
      "[4,    40] loss: 0.4546098\n",
      "[5,    40] loss: 0.4478408\n",
      "[6,    40] loss: 0.4374063\n",
      "[7,    40] loss: 0.4322670\n",
      "[8,    40] loss: 0.4295413\n",
      "[9,    40] loss: 0.4269801\n",
      "[10,    40] loss: 0.4254086\n",
      "[11,    40] loss: 0.4234504\n",
      "[12,    40] loss: 0.4223551\n",
      "[13,    40] loss: 0.4205424\n",
      "[14,    40] loss: 0.4197156\n",
      "[15,    40] loss: 0.4180180\n",
      "[16,    40] loss: 0.4174608\n",
      "[17,    40] loss: 0.4156563\n",
      "[18,    40] loss: 0.4157127\n",
      "[19,    40] loss: 0.4139317\n",
      "[20,    40] loss: 0.4136942\n",
      "[21,    40] loss: 0.4122748\n",
      "[22,    40] loss: 0.4124200\n",
      "[23,    40] loss: 0.4110222\n",
      "[24,    40] loss: 0.4110186\n",
      "[25,    40] loss: 0.4100516\n",
      "[26,    40] loss: 0.4103960\n",
      "[27,    40] loss: 0.4094151\n",
      "[28,    40] loss: 0.4091370\n",
      "[29,    40] loss: 0.4091751\n",
      "[30,    40] loss: 0.4084966\n",
      "[31,    40] loss: 0.4073048\n",
      "[32,    40] loss: 0.4139395\n",
      "[33,    40] loss: 0.4129250\n",
      "[34,    40] loss: 0.4081255\n",
      "[35,    40] loss: 0.4080434\n",
      "[36,    40] loss: 0.4078882\n",
      "[37,    40] loss: 0.4077653\n",
      "[38,    40] loss: 0.4076759\n",
      "[39,    40] loss: 0.4075894\n",
      "Finished Training\n",
      "Error rate of test images: 0.178571\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader7, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer7.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn7(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer7.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader7)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader7:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn7(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader7):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a9784",
   "metadata": {},
   "source": [
    "***train(300)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f76e8014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,    40] loss: 17.9575556\n",
      "[1,    40] loss: 9.9492317\n",
      "[2,    40] loss: 1.4843659\n",
      "[3,    40] loss: 1.0873458\n",
      "[4,    40] loss: 1.0584452\n",
      "[5,    40] loss: 0.6174273\n",
      "[6,    40] loss: 0.5741002\n",
      "[7,    40] loss: 0.5254247\n",
      "[8,    40] loss: 0.4919423\n",
      "[9,    40] loss: 0.4694286\n",
      "[10,    40] loss: 0.4547379\n",
      "[11,    40] loss: 0.4454986\n",
      "[12,    40] loss: 0.4398219\n",
      "[13,    40] loss: 0.4363922\n",
      "[14,    40] loss: 0.4343461\n",
      "[15,    40] loss: 0.4331256\n",
      "[16,    40] loss: 0.4323969\n",
      "[17,    40] loss: 0.4319758\n",
      "[18,    40] loss: 0.4317460\n",
      "[19,    40] loss: 0.4316218\n",
      "[20,    40] loss: 0.4315746\n",
      "[21,    40] loss: 0.4315723\n",
      "[22,    40] loss: 0.4316002\n",
      "[23,    40] loss: 0.4316525\n",
      "[24,    40] loss: 0.4317270\n",
      "[25,    40] loss: 0.4318016\n",
      "[26,    40] loss: 0.4318836\n",
      "[27,    40] loss: 0.4319615\n",
      "[28,    40] loss: 0.4320462\n",
      "[29,    40] loss: 0.4321330\n",
      "[30,    40] loss: 0.4322222\n",
      "[31,    40] loss: 0.4322924\n",
      "[32,    40] loss: 0.4323828\n",
      "[33,    40] loss: 0.4324553\n",
      "[34,    40] loss: 0.4325258\n",
      "[35,    40] loss: 0.4326242\n",
      "[36,    40] loss: 0.4326941\n",
      "[37,    40] loss: 0.4327710\n",
      "[38,    40] loss: 0.4328346\n",
      "[39,    40] loss: 0.4329048\n",
      "Finished Training\n",
      "Error rate of test images: 0.142857\n",
      "AE error rate : 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader8, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer8.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn8(inputs)\n",
    "        cnn_loss = cnn_criterion(outputs, labels)\n",
    "        cnn_loss.backward()\n",
    "        optimizer8.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += cnn_loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.7f' %(epoch, num_epochs, running_loss/len(train_loader8)))\n",
    "    \n",
    "print('Finished Training')\n",
    "    \n",
    "with torch.no_grad():\n",
    "    error = 0\n",
    "    total = 0\n",
    "    for data in test_loader8:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = cnn8(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "cnn_err_rate.append(error/total)\n",
    "print('Error rate of test images: %f' % (error / total))\n",
    "\n",
    "\n",
    "ae_err = 0\n",
    "for i, (X_abnormal, _) in enumerate(abnormal_loader8):\n",
    "    X_abnormal = X_abnormal.cuda()\n",
    "    output = AE(X_abnormal)\n",
    "    ae_loss = ae_criterion(X_abnormal, output)\n",
    "        \n",
    "    if ae_loss.item() < threshold:\n",
    "        ae_err += 1\n",
    "    \n",
    "ae_err_rate.append(ae_err)\n",
    "print('AE error rate : %.6f ' %(ae_err/len(abnormal_loader8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13bfa7",
   "metadata": {},
   "source": [
    "***graph***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7f8bc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_abnormal = [10, 30, 60, 90, 120, 150, 230, 300]\n",
    "\n",
    "plt.plot(num_abnormal, cnn_err_rate, label = 'CNN')\n",
    "plt.plot(num_abnormal, ae_err_rate, label = 'CAE')\n",
    "plt.xlabel('Number of abnormal data')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
